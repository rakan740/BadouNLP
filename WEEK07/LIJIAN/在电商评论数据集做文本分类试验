import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
import torch
import time


# æ•°æ®åŠ è½½ä¸é¢„å¤„ç†
def load_and_preprocess_data(file_path):
    df = pd.read_csv(file_path)
    
    # æ•°æ®ç»Ÿè®¡åˆ†æ
    class_counts = df['sentiment'].value_counts()
    text_lengths = df['text'].str.split().str.len()
    
    print("\nğŸ“Š æ•°æ®ç»Ÿè®¡æŠ¥å‘Š")
    print(f"æ€»æ ·æœ¬æ•°: {len(df)}")
    print(f"æ­£æ ·æœ¬æ•°: {class_counts[1]} | è´Ÿæ ·æœ¬æ•°: {class_counts[0]}")
    print(f"æ–‡æœ¬å¹³å‡é•¿åº¦: {text_lengths.mean():.2f} tokens")
    print(f"æ–‡æœ¬é•¿åº¦æ ‡å‡†å·®: {text_lengths.std():.2f}")
    
    return df

# æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°åŸºç±»
class SentimentModel:
    def __init__(self, model_name, tokenizer_name):
        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
        self.model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
    def tokenize(self, texts):
        return self.tokenizer(texts.tolist(), 
                            padding=True,
                            truncation=True,
                            max_length=512,
                            return_tensors='pt')
    
    def train(self, train_dataset, val_dataset):
        training_args = TrainingArguments(
            output_dir='./results',
            num_train_epochs=3,
            per_device_train_batch_size=8,
            per_device_eval_batch_size=16,
            evaluation_strategy="epoch",
            save_strategy="epoch",
            load_best_model_at_end=True
        )
        
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
